{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90e213b",
   "metadata": {},
   "source": [
    "### Pre-Tokenization using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e382a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o200k_base\n",
      "[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "model = \"gpt-5\"\n",
    "enc = tiktoken.encoding_for_model(model)\n",
    "\n",
    "print(enc.name)\n",
    "print(enc._pat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a197f88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' world', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "regex_pattern = enc._pat_str\n",
    "text = \"Hello, world!\"\n",
    "re.findall(regex_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c6725f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Iterable, Optional\n",
    "import regex as re\n",
    "\n",
    "IntSeq = tuple[int, ...]\n",
    "Pair = tuple[int, int]\n",
    "\n",
    "class BPE():\n",
    "    def __init__(self, pat_str, target_merges: int=3, vocab_size:int=512, special_tokens: Optional[list[str]]=None):\n",
    "        self.pat_str = pat_str\n",
    "        self.regex = re.compile(self.pat_str)\n",
    "        self.merges: list[tuple[Pair, int]] = []\n",
    "        self.ranks: dict[Pair, int] = {}    \n",
    "        self.next_id: int  = 256\n",
    "        self.target_merges: int = target_merges\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.encoder: dict[IntSeq, int] = {(i,): i for i in range(256)}\n",
    "        self.decoder: dict[int, IntSeq] = {i: (i,) for i in range(256)}\n",
    "        self.special_tokens: list[str] = special_tokens or  []\n",
    "        self.special_token_to_id: dict[str, int] = {}\n",
    "        self.add_special_token()\n",
    "\n",
    "    def add_special_token(self) -> None:\n",
    "        for token_str in self.special_tokens:\n",
    "            token_int: IntSeq = tuple(token_str.encode(\"utf-8\"))\n",
    "            if token_int in self.encoder:\n",
    "                token_id = self.encoder[token_int]\n",
    "            else:\n",
    "                token_id = self.next_id\n",
    "                self.encoder[token_int] = token_id\n",
    "                self.decoder[token_id] = token_int\n",
    "                self.next_id += 1\n",
    "            self.special_token_to_id[token_str] = token_id\n",
    "\n",
    "    def decode_to_bytes(self, token_ids: list[int]) -> bytes:\n",
    "            return b\"\".join(bytes(self.decoder[token_id]) for token_id in token_ids)\n",
    "    \n",
    "    def decode_text(self, token_ids: list[int]) -> str:\n",
    "        return self.decode_to_bytes(token_ids).decode(\"utf-8\", errors=\"strict\")\n",
    "    \n",
    "    def pre_tokenize(self, text: str) -> list[str]:\n",
    "        return [m.group(0) for m in self.regex.finditer(text)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_all_pairs(tokens: list[int]) -> Iterable[Pair]:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            yield (tokens[i], tokens[i+1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def text_to_ints(text: str) -> list[int]:\n",
    "        return list(text.encode('utf-8'))\n",
    "\n",
    "    def _replace_pair(self, chunks: list[list[int]], pair: Pair) -> list[list[int]]:\n",
    "        def go_to_next_seq(index: int) -> int:\n",
    "            return index + 1\n",
    "\n",
    "        def add_current_seq_to_replaced_chunk(index: int, replaced_chunk: list[int]) -> list[int]:\n",
    "            replaced_chunk.append(chunk[index])\n",
    "            return replaced_chunk\n",
    "\n",
    "        def skip_next_byte(index: int) -> int:\n",
    "            return index + 2\n",
    "\n",
    "\n",
    "        def not_end_of_chunks(i: int, chunk: list[int]) -> bool:\n",
    "            return i + 1 < len(chunk)\n",
    "\n",
    "        def pair_at_index(chunk: list[int], pair: Pair, index: int) -> bool:\n",
    "            return chunk[index] == pair[0] and chunk[index + 1] == pair[1]\n",
    "\n",
    "        def found_pair_at_index(i: int, chunk: list[int], pair: Pair) -> bool:\n",
    "            return not_end_of_chunks(i, chunk) and pair_at_index(chunk, pair, i)\n",
    "\n",
    "        def replace_pair_with_merged(replaced_chunk: list[int]) -> list[int]:\n",
    "            new_id = self.next_id - 1\n",
    "            replaced_chunk.append(new_id)\n",
    "            return replaced_chunk\n",
    "\n",
    "        def replace_pair_in_chunk(chunk: list[int], pair: Pair) -> list[int]:\n",
    "            replaced_chunk: list[int] = []\n",
    "            i: int = 0\n",
    "            while i < len(chunk):\n",
    "                if found_pair_at_index(i, chunk, pair):\n",
    "                    replaced_chunk = replace_pair_with_merged(replaced_chunk)\n",
    "                    i = skip_next_byte(i)\n",
    "                else:\n",
    "                    replaced_chunk = add_current_seq_to_replaced_chunk(i, replaced_chunk)\n",
    "                    i = go_to_next_seq(i)\n",
    "            return replaced_chunk\n",
    "        \n",
    "        replaced_chunks: list[list[int]] = []\n",
    "        for chunk in chunks:\n",
    "            replaced_chunk: list[int] = replace_pair_in_chunk(chunk, pair)\n",
    "            replaced_chunks.append(replaced_chunk)\n",
    "        return replaced_chunks\n",
    "    \n",
    "    def _count_pairs_frequency(self, chunks: list[list[int]]) -> Counter[Pair]:\n",
    "        def chunk_to_small_for_merging(chunk: list[int]) -> bool:\n",
    "            return len(chunk) < 2\n",
    "\n",
    "        global_counter: Counter[Pair] = Counter()\n",
    "        for chunk in chunks:\n",
    "            if chunk_to_small_for_merging(chunk): continue\n",
    "            pairs: zip[int, int] = self._get_all_pairs(chunk)\n",
    "            global_counter.update(pairs)\n",
    "        return global_counter\n",
    "\n",
    "\n",
    "    def _get_most_frequent_pair(self, chunks: list[list[int]]) -> Optional[Pair]:\n",
    "        def tie_breaker_key(kv: tuple[Pair, int]) -> tuple[int, Pair]:\n",
    "            pair, freq = kv\n",
    "            resulting = self.decoder[pair[0]] + self.decoder[pair[1]]\n",
    "            return (-freq, resulting, pair)\n",
    "\n",
    "        def hasnt_pairs() -> bool:\n",
    "            return not global_counter\n",
    "        \n",
    "        global_counter: Counter[Pair] = self._count_pairs_frequency(chunks)\n",
    "        if hasnt_pairs(): return None\n",
    "        best_pair, _ = min(global_counter.items(), key=lambda kv: tie_breaker_key(kv))\n",
    "        return best_pair\n",
    "\n",
    "    def _register_merge(self, pair: Pair, merges_done: int):\n",
    "        def token_already_exists() -> bool:\n",
    "            return merged_token in self.encoder\n",
    "\n",
    "        first_token: IntSeq = self.decoder[pair[0]]\n",
    "        second_token: IntSeq = self.decoder[pair[1]]\n",
    "        merged_token: IntSeq = first_token + second_token\n",
    "\n",
    "        if token_already_exists(): return\n",
    "        \n",
    "        self.encoder[merged_token] = self.next_id\n",
    "        self.decoder[self.next_id] = merged_token\n",
    "\n",
    "        self.merges.append((pair, self.next_id))\n",
    "        self.next_id += 1\n",
    "        self.ranks[pair] = merges_done\n",
    "\n",
    "    def train(self, corpus: str):\n",
    "        def not_enough_merges_done() -> bool:\n",
    "            nonlocal merges_done\n",
    "            return merges_done < self.target_merges\n",
    "            \n",
    "        pre_tokens: list[str] = self.pre_tokenize(corpus)\n",
    "        chunks: list[list[int]] = [self.text_to_ints(token) for token in pre_tokens]\n",
    "\n",
    "        max_merges: int = max(0, self.vocab_size - self.next_id)\n",
    "        self.target_merges: int = min(self.target_merges, max_merges)\n",
    "\n",
    "        merges_done: int = 0\n",
    "        while not_enough_merges_done():\n",
    "            best_pair: Optional[Pair] = self._get_most_frequent_pair(chunks)\n",
    "            if best_pair is None: break\n",
    "            self._register_merge(best_pair, merges_done)\n",
    "            chunks: list[list[int]] = self._replace_pair(chunks, best_pair)\n",
    "            \n",
    "            merges_done += 1\n",
    "\n",
    "        print(\"Training complete.\"\n",
    "              f\" Total merges done: {merges_done}.\"\n",
    "              f\" Vocabulary size: {256 + len(self.merges)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "5d709c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Total merges done: 3. Vocabulary size: 259.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPE(pat_str=enc._pat_str)\n",
    "tokenizer.train(corpus=\"Helloll, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "[b'H', b'e', b'l', b'l', b'o']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
