{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b37b072",
   "metadata": {},
   "source": [
    "# Texto to N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976131ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'an', 'IA', 'student.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_gram(text, n_grams=1):\n",
    "    sequence = text.split()\n",
    "    n_gram_list = []\n",
    "    for i in range(len(sequence) - n_grams + 1):\n",
    "        n_gram_list.append(' '.join(sequence[i:i + n_grams]))\n",
    "    return n_gram_list\n",
    "\n",
    "n_gram(\"I am an IA student.\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796ce87",
   "metadata": {},
   "source": [
    "### Train a n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/matheus/miniconda3/envs/NLP/lib/python3.14/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/matheus/miniconda3/envs/NLP/lib/python3.14/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/matheus/miniconda3/envs/NLP/lib/python3.14/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/matheus/miniconda3/envs/NLP/lib/python3.14/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /home/matheus/miniconda3/envs/NLP/lib/python3.14/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f26435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/matheus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/matheus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/matheus/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589f6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class SpecialToken(Enum):\n",
    "    START_TOKEN = '<SOT>'\n",
    "    END_TOKEN = '<EOT>'\n",
    "    UNK_TOKEN = '<UNK>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61107900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text: the fogge and filthie ayre.\n",
      "\n",
      "Exeunt.\n",
      "\n",
      "\n",
      "Scena Secunda.\n",
      "\n",
      "Alarum within. Enter King Malcome, Donalbaine\n",
      "Normalized Text: the fogge and filthie ayre . exeunt . scena secunda . alarum within . enter king malcome , donalbaine\n"
     ]
    }
   ],
   "source": [
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    normalized_text = \" \".join(tokens)\n",
    "    return normalized_text\n",
    "\n",
    "fileid = 'shakespeare-macbeth.txt'\n",
    "raw_text = gutenberg.raw(fileid)\n",
    "print(\"Raw Text:\", raw_text[500:600])\n",
    "normalized_text = normalize_text(raw_text[500:600])\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61142b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.vocab = set([SpecialToken.UNK_TOKEN.value, SpecialToken.START_TOKEN.value, SpecialToken.END_TOKEN.value])\n",
    "        self.tokenized_corpus = []\n",
    "        self.ngram_counts = Counter() \n",
    "        self.context_counts = Counter() \n",
    "\n",
    "    def tokenizer(self, text, padding=True):\n",
    "        tokens = word_tokenize(text)\n",
    "        start_tokens = [SpecialToken.START_TOKEN.value]\n",
    "        if padding:\n",
    "            start_tokens = start_tokens * (self.n - 1)\n",
    "        return start_tokens + tokens + [SpecialToken.END_TOKEN.value]\n",
    "\n",
    "    def create_vocab(self):\n",
    "        self.vocab.update(set(self.tokenized_corpus))\n",
    "\n",
    "    def compute_probabilities(self, candidates):\n",
    "        if not isinstance(candidates, tuple):\n",
    "            candidates = tuple(candidates)\n",
    "\n",
    "        context = candidates[0][:-1]\n",
    "        context_count = self.context_counts[tuple(context)]\n",
    "        candidates_count = {candidate[-1]: self.ngram_counts[tuple(candidate)] for candidate in candidates}\n",
    "        probabilities = {word: count / context_count if context_count > 0 else 0 for word, count in candidates_count.items()}\n",
    "        return probabilities\n",
    "    \n",
    "    def collect_ngram_counts(self):\n",
    "        for i in range(self.n - 1, len(self.tokenized_corpus)):\n",
    "            ngram = tuple(self.tokenized_corpus[i - self.n + 1 : i + 1])\n",
    "            context = tuple(ngram[:-1])\n",
    "            self.ngram_counts[ngram] += 1\n",
    "            self.context_counts[context] += 1\n",
    "\n",
    "    def predict_next_token(self, token_sequence):\n",
    "        start_context_index = max(0, len(token_sequence) - self.n +1 )\n",
    "        context = token_sequence[start_context_index:]\n",
    "        if len(context) < self.n - 1:\n",
    "            pad = [SpecialToken.START_TOKEN.value] * (self.n - 1 - len(context))\n",
    "            context = pad + context\n",
    "        candidates = [context + [word] for word in self.vocab]\n",
    "        probabilities = self.compute_probabilities(candidates)\n",
    "        return max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    def generate_text(self, seed_text, max_length=20):\n",
    "        tokenized_seed = word_tokenize(seed_text)\n",
    "        generated_tokens = tokenized_seed.copy()\n",
    "        for _ in range(max_length):\n",
    "            next_token = self.predict_next_token(generated_tokens)\n",
    "            if next_token == SpecialToken.END_TOKEN.value:\n",
    "                break\n",
    "            generated_tokens.append(next_token)\n",
    "        return ' '.join(generated_tokens)\n",
    "    \n",
    "    def fit(self, text):\n",
    "        self.tokenized_corpus = self.tokenizer(text, padding=True)\n",
    "        self.create_vocab()\n",
    "        self.collect_ngram_counts()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78acb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the tragedie of macbeth . macb . i , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead , and the dead'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramModel(3)\n",
    "normalized_text = normalize_text(raw_text)\n",
    "model.fit(text=normalized_text)\n",
    "print(model.predict_next_token(word_tokenize(\"the tragedie\")))\n",
    "model.generate_text(\"the tragedie\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8a047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
